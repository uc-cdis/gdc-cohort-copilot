{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import yaml\n",
    "import hashlib\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_json_string(json_str):\n",
    "    try:\n",
    "        obj = json.loads(json_str)\n",
    "        normalized_str = json.dumps(obj, sort_keys=True)\n",
    "        return normalized_str\n",
    "    except Exception:\n",
    "        # Handle invalid JSON strings if necessary\n",
    "        return json_str\n",
    "\n",
    "def hash_json_string(json_str):\n",
    "    normalized_str = normalize_json_string(json_str)\n",
    "    return hashlib.md5(normalized_str.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# filter deduplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_100k_v2.tsv\", sep=\"\\t\")\n",
    "print(len(df))\n",
    "\n",
    "# Create a hash column\n",
    "df['json_hash'] = df['filters'].apply(hash_json_string)\n",
    "\n",
    "# Drop duplicates based on the hash column\n",
    "df_dedup_100k = df.drop_duplicates(subset='json_hash').copy()\n",
    "\n",
    "print(len(df_dedup_100k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"deficit in 100k dataset: {100_000 - len(df_dedup_100k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_1M_v1.tsv\", sep=\"\\t\")\n",
    "print(len(df))\n",
    "\n",
    "# Create a hash column\n",
    "df['json_hash'] = df['filters'].apply(hash_json_string)\n",
    "\n",
    "# Drop duplicates based on the hash column\n",
    "df_dedup_1M = df.drop_duplicates(subset='json_hash').copy()\n",
    "print(len(df_dedup_1M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"deficit in 1M dataset: {1_000_000 - len(df_dedup_1M)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the overlap in the two datasets\n",
    "\n",
    "intersection = set(df_dedup_100k[\"json_hash\"].tolist()).intersection(set(df_dedup_1M[\"json_hash\"].tolist()))\n",
    "print(len(intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross_dataset_dupes = df_dedup_100k[df_dedup_100k[\"json_hash\"].isin(list(intersection))]\n",
    "df_cross_dataset_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 70\n",
    "json.loads(df_cross_dataset_dupes.iloc[i][\"filters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deficit\n",
    "\n",
    "df_deficit = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/deficit_samples_v1.tsv\", sep=\"\\t\")\n",
    "print(len(df_deficit))\n",
    "# Create a hash column\n",
    "df_deficit['json_hash'] = df_deficit['filters'].apply(hash_json_string)\n",
    "\n",
    "# Drop duplicates based on the hash column\n",
    "df_deficit_dedup = df_deficit.drop_duplicates(subset='json_hash').copy()\n",
    "df_deficit_dedup[:30_000].to_csv(\"/opt/gpudata/gdc-eval/results/datasets/deficit_samples_v2.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/deficit_samples_v2.tsv\", sep=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup_100k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# naive sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/opt/gpudata/anirudh/git-repos/gdc-eval/ref-data/fields_short_v2.yaml\", \"r\"\n",
    ") as f:\n",
    "    cbf_mappings = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cbf_mappings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cbf_mappings.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 6  # degrees of freedom, also the mean of the chi-squared distribution\n",
    "num_samples = 1_000_000\n",
    "margin = 0.15\n",
    "samples_margin = np.round(np.random.chisquare(df=df, size=int(num_samples + num_samples*margin))).astype(\n",
    "    int\n",
    ")  # sample from chi-squared and round to nearest integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples_margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [samples_margin[i] for i in list(samples_margin.nonzero()[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_100k = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_100k_v2.tsv\", sep=\"\\t\")\n",
    "df_samples_100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "100000 / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_100k_duplicates = df_samples_100k[df_samples_100k[\"filters\"].duplicated(keep=False)]\n",
    "df_samples_100k_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_100k_duplicates.iloc[0][\"filters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_100k_duplicates.iloc[1][\"filters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_100k_duplicates.iloc[2][\"filters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_samples_100k\n",
    "filter_col = \"filters\"\n",
    "df = df[df[filter_col] != \"{}\"]\n",
    "print(len(df))\n",
    "df = df.drop_duplicates(subset=[filter_col])\n",
    "print(len(df))\n",
    "# remove examples containing set_id\n",
    "df[\"set_id\"] = df[filter_col].str.contains(\"set_id\")\n",
    "df = df[df[\"set_id\"] == False]\n",
    "print(len(df))\n",
    "# remove examples containing case_id\n",
    "df[\"case_id\"] = df[filter_col].str.contains(\"case_id\")\n",
    "df = df[df[\"case_id\"] == False]\n",
    "print(len(df))\n",
    "# remove examples containing gene_id\n",
    "df[\"gene_id\"] = df[filter_col].str.contains(\"gene_id\")\n",
    "df = df[df[\"gene_id\"] == False]\n",
    "print(len(df))\n",
    "# remove examples containing ssm_id\n",
    "df[\"ssm_id\"] = df[filter_col].str.contains(\"ssm_id\")\n",
    "df = df[df[\"ssm_id\"] == False]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_filter_dataset(df, filter_col):\n",
    "    df = df[df[filter_col] != \"{}\"]\n",
    "    df = df.drop_duplicates(subset=[filter_col])\n",
    "    # remove examples containing set_id\n",
    "    df[\"set_id\"] = df[filter_col].str.contains(\"set_id\")\n",
    "    df = df[df[\"set_id\"] == False]\n",
    "    # remove examples containing case_id\n",
    "    df[\"case_id\"] = df[filter_col].str.contains(\"case_id\")\n",
    "    df = df[df[\"case_id\"] == False]\n",
    "    # remove examples containing gene_id\n",
    "    df[\"gene_id\"] = df[filter_col].str.contains(\"gene_id\")\n",
    "    df = df[df[\"gene_id\"] == False]\n",
    "    # remove examples containing ssm_id\n",
    "    df[\"ssm_id\"] = df[filter_col].str.contains(\"ssm_id\")\n",
    "    df = df[df[\"ssm_id\"] == False]\n",
    "\n",
    "    def extract_logged(example):\n",
    "        d = json.loads(example)\n",
    "        if \"isLoggedIn\" in d:\n",
    "            d.pop(\"isLoggedIn\")\n",
    "        return str(d)\n",
    "\n",
    "    # remove 'isLoggedIn' key from filter dict\n",
    "    df[\"filters_cleaned\"] = df[\"filters\"].apply(extract_logged)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_filter_100k = _prepare_filter_dataset(df_samples_100k, \"filters\")\n",
    "df_filter_100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_indices = set(df_samples_100k.index.to_list()) - set(df_filter_100k.index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_df_100k = df_samples_100k.iloc[sorted(list(missed_indices))]\n",
    "missed_df_100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in missed_df_100k.iterrows():\n",
    "    print(json.loads(row[\"filters\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# generated queries exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/opt/gpudata/gdc-eval/results/datasets/Mistral-7B-Instruct-v0.3_generated_queries_100k_naive_v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[i][\"filters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.iloc[i][\"filters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ast.literal_eval(df.iloc[i][\"filters\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(json.dumps(ast.literal_eval(df.iloc[i][\"filters\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = ast.literal_eval(df.iloc[i][\"filters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_endpt = 'https://api.gdc.cancer.gov/cases'\n",
    "params = {\n",
    "  \"filters\" : json.dumps(filter),\n",
    "  \"pretty\" : \"false\",\n",
    "}\n",
    "response = requests.get(cases_endpt, params = params)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_1M_v1_part1.tsv\", sep=\"\\t\")\n",
    "len(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = df_1.iloc[0][\"filters\"]\n",
    "print(example1)\n",
    "print(type(example1))\n",
    "print(json.loads(example1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_endpt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_1M_v1_part2.tsv\", sep=\"\\t\")\n",
    "len(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_1M_v1_part3.tsv\", sep=\"\\t\")\n",
    "len(df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "# Train Test set exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/opt/gpudata/steven/gdc-cohort-pilot/data/train.csv\")\n",
    "test_df = pd.read_csv(\"/opt/gpudata/steven/gdc-cohort-pilot/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[97][\"filters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0][\"filters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(train_df.iloc[0][\"filters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "example = {'op': 'and',\n",
    " 'content': [{'op': 'in',\n",
    "   'content': {'field': 'project.program.name', 'value': ['CPTAC']}}]}\n",
    "projects_endpt = 'https://api.gdc.cancer.gov/cases'\n",
    "params = {\n",
    "    'filters': json.dumps(example),\n",
    "    'size' : 1687,\n",
    "    }\n",
    "response = requests.get(projects_endpt, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set_ = set([hit.get(\"id\") for hit in response.json()[\"data\"][\"hits\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_endpt = 'https://api.gdc.cancer.gov/cases'\n",
    "example = {'op': 'and',\n",
    " 'content': [{'op': 'in',\n",
    "   'content': {'field': 'cases.project.program.name', 'value': ['CPTAC']}}]}\n",
    "params = {\n",
    "    'filters': json.dumps(example),\n",
    "    'size': 1687,\n",
    "        }\n",
    "response = requests.get(projects_endpt, params = params)\n",
    "# print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set_cases = set([hit.get(\"id\") for hit in response.json()[\"data\"][\"hits\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set_ - id_set_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_to_fields(json_str, prefix):\n",
    "    def recurse(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if key == \"field\" and isinstance(value, str):\n",
    "                    obj[key] = prefix + value\n",
    "                else:\n",
    "                    recurse(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                recurse(item)\n",
    "        # Ignore primitives like str/int/None/etc.\n",
    "\n",
    "    data = json.loads(json_str)\n",
    "    recurse(data)\n",
    "    return data\n",
    "\n",
    "def modify_json_string(json_str):\n",
    "    try:\n",
    "        return json.dumps(prepend_to_fields(json_str, \"cases.\"))\n",
    "    except Exception as e:\n",
    "        return None  # or log the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100k = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_100k_v2.tsv\", sep=\"\\t\")\n",
    "df_100k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100k['corrected_filters'] = df_100k['filters'].apply(modify_json_string)\n",
    "df_100k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rand_indices = random.sample(range(0, 100001), 10)\n",
    "\n",
    "for i, row in df_100k.iterrows():\n",
    "    print(json.loads(row[\"filters\"]))\n",
    "    print(json.loads(row[\"corrected_filters\"]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    if i == 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/Mistral-7B-Instruct-v0.3_generated_queries_100k_naive_v2.csv\")\n",
    "df_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rand_indices = random.sample(range(0, 100001), 25)\n",
    "\n",
    "for i in rand_indices:\n",
    "    print(i)\n",
    "    print(json.loads(df_100k.iloc[i][\"filters\"]))\n",
    "    print(json.loads(df_queries.iloc[i][\"filters\"]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check old mistral rewrites \n",
    "\n",
    "rew_df = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/Mistral-7B-Instruct-v0.3_generated_queries_v2.csv\")\n",
    "rew_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "# Check dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk(\"/opt/gpudata/gdc-eval/results/datasets/gdc_eval_train_tokenized.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data[\"raw\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"/opt/gpudata/gdc-cohort-data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.map(lambda example: {\"prompt\":example[\"queries\"].strip(),\"completion\":example[\"filters\"]}, remove_columns=[\"filters\", \"queries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"left\"  # needed for flash attention\n",
    "ds = ds.map(lambda example: {\"text\": f\"{example['prompt']}{example['completion']}\"})\n",
    "def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                max_length=1024,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "            )\n",
    "tok_dataset = ds[\"train\"].select(range(64)).map(tokenize_function, batched=True, remove_columns=[\"prompt\", \"completion\"])\n",
    "tok_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrites_df =pd.read_csv(\"/opt/gpudata/gdc-cohort-data/user_cohort_rewrites_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrites_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "# Dataset Corrections : field prepend endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_to_fields(json_str, prefix):\n",
    "    def recurse(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if key == \"field\" and isinstance(value, str):\n",
    "                    obj[key] = prefix + value\n",
    "                else:\n",
    "                    recurse(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                recurse(item)\n",
    "        # Ignore primitives like str/int/None/etc.\n",
    "\n",
    "    # data = json.loads(json_str)\n",
    "    data = ast.literal_eval(json_str)\n",
    "    recurse(data)\n",
    "    return data\n",
    "\n",
    "def extract_fields(json_str):\n",
    "    fields = []\n",
    "\n",
    "    def recurse(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if key == \"field\" and isinstance(value, str):\n",
    "                    fields.append(value)\n",
    "                else:\n",
    "                    recurse(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                recurse(item)\n",
    "\n",
    "    data = json.loads(json_str)\n",
    "    recurse(data)\n",
    "    return fields\n",
    "\n",
    "\n",
    "def modify_json_string(json_str):\n",
    "    try:\n",
    "        return json.dumps(prepend_to_fields(json_str, \"cases.\"))\n",
    "    except Exception as e:\n",
    "        return None  # or log the error\n",
    "\n",
    "def all_fields_prefixed(obj, prefix):\n",
    "    success = True\n",
    "\n",
    "    def recurse(o):\n",
    "        nonlocal success\n",
    "        if isinstance(o, dict):\n",
    "            for k, v in o.items():\n",
    "                if k == \"field\" and isinstance(v, str):\n",
    "                    if not v.startswith(prefix):\n",
    "                        success = False\n",
    "                else:\n",
    "                    recurse(v)\n",
    "        elif isinstance(o, list):\n",
    "            for item in o:\n",
    "                recurse(item)\n",
    "\n",
    "    recurse(obj)\n",
    "    return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deficit = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/deficit_samples_v2_queries.tsv\")\n",
    "df_deficit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100k synthethic correction\n",
    "df_100k = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_100k_v2_queries.csv\")\n",
    "df_100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrected_100k = df_100k[[\"filters\", \"queries\"]]\n",
    "df_corrected_100k = df_corrected_100k.rename(columns={\"filters\": \"filters_og\"})\n",
    "df_corrected_100k[\"filters\"] = df_corrected_100k[\"filters_og\"].apply(modify_json_string)\n",
    "df_corrected_100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrected_100k[\"is_valid\"] = df_corrected_100k[\"filters\"].apply(lambda s: all_fields_prefixed(ast.literal_eval(s), 'cases.'))\n",
    "df_corrected_100k[~df_corrected_100k[\"is_valid\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100k merging\n",
    "n1 = 100_000 - len(df_corrected_100k)\n",
    "df1_slice = df_corrected_100k[[\"filters\", \"queries\"]]\n",
    "df2_slice = df_deficit[:n1]\n",
    "df_100k_full = pd.concat([df1_slice, df2_slice], axis=0, ignore_index=True).drop(columns=\"prompts\")\n",
    "df_100k_full.to_csv(\"/opt/gpudata/gdc-cohort-data/train_synthetic_100k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users + 100k merging\n",
    "df_users = pd.read_csv(\"/opt/gpudata/gdc-cohort-data/train.csv\")\n",
    "# users + 100k synthetic\n",
    "df_u_100k = pd.concat([df_users, df_100k_full], axis=0, ignore_index=True)\n",
    "df_u_100k.to_csv(\"/opt/gpudata/gdc-cohort-data/train_synthetic_users+100k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1M synthethic \n",
    "df_1M_part1 = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_1M_v1_part1_queries.csv\")\n",
    "df_1M_part2 = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_1M_v1_part2_queries.csv\")\n",
    "df_1M_part3 = pd.read_csv(\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_1M_v1_part3_queries.csv\")\n",
    "\n",
    "df_1M = pd.concat([df_1M_part1, df_1M_part2, df_1M_part3], axis=0, ignore_index=True)\n",
    "df_1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrected_1M = df_1M[[\"filters\", \"queries\"]]\n",
    "df_corrected_1M = df_corrected_1M.rename(columns={\"filters\": \"filters_og\"})\n",
    "df_corrected_1M[\"filters\"] = df_corrected_1M[\"filters_og\"].apply(modify_json_string)\n",
    "df_corrected_1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrected_1M[\"is_valid\"] = df_corrected_1M[\"filters\"].apply(lambda s: all_fields_prefixed(ast.literal_eval(s), 'cases.'))\n",
    "df_corrected_1M[~df_corrected_1M[\"is_valid\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1M merging\n",
    "n2 = 1000_000 - len(df_corrected_1M)\n",
    "df1_slice = df_corrected_1M\n",
    "df2_slice = df_deficit[n1:(n1+n2)]\n",
    "df_1M_full = pd.concat([df1_slice, df2_slice], axis=0, ignore_index=True)\n",
    "df_1M_full.to_csv(\"/opt/gpudata/gdc-cohort-data/train_synthetic_1M.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1M_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users + 100k merging\n",
    "df_users = pd.read_csv(\"/opt/gpudata/gdc-cohort-data/train.csv\")\n",
    "# users + 100k synthetic\n",
    "df_u_1M = pd.concat([df_users, df_1M_full], axis=0, ignore_index=True).drop(columns=[\"filters_og\", \"is_valid\", \"prompts\"])\n",
    "df_u_1M.to_csv(\"/opt/gpudata/gdc-cohort-data/train_synthetic_users+1M.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "# training script updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\":\"/opt/gpudata/gdc-eval/results/datasets/naive_sampler_100k_v2_queries.csv\"})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        lambda example: {\n",
    "            \"prompt\": example[\"queries\"].strip(),\n",
    "            \"completion\": example[\"filters\"],\n",
    "        },\n",
    "        remove_columns=[\"filters\", \"prompts\", \"queries\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda example: {\"text\":f\"{example['prompt']}{example['completion']}\"})\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cohort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
